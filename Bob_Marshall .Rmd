---
title: Bob_Marshall Excercise
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mice)
BMWC_1982_Data <- read_csv("RDS-2017-0015/Data/BMWC_1982_Data.csv")
Marshall <- BMWC_1982_Data
```

##Bob Marshall Machine Learning Excercise 
For this project I wanted to explore machine learning in R with a real dataset. I am also a huge fan of all things ecology, I enjoy hiking and generally being in the outdoors. From the USDA.gov website, there is a page dedicated to Forest Service Research and a group of researchers collected survey data from visitors to the the Bob Marshall Wilderness Complex in 1982. 

#Data Selecting and Formatting
Let's start by taking a look at our dataset. 
```{r}
Marshall %>% head()
Marshall %>% length()
Marshall %>% count()
sum(is.na(Marshall))
```
From the above code, we can see we have 181 variables, 746 obserations, and it looks like most of our variables are encoded as 0s and 1s. 

I know my output variable will be a satisfaction score that is rated 1 = best, 5 is worst and 9 means missing. For this project, I am just going to categorize the variable into two categories,TRUE, FALSE and call it "satisfied", all 1s will be TRUE and anything else will be FALSE and 9s will be excluded from analysis. 
```{r}
Marshall <- Marshall%>%
  mutate(satisfied = as.factor(ifelse(SATIS == 1, TRUE, FALSE)))%>%
  filter(SATIS != 9)
Marshall$SATIS <- NULL
```
Now for the missing data
```{r}
convertnines <- function(x){
  x <- ifelse(x == 99 | x == 9 | x == 999, NA, x)
}

Marshall <- as.data.frame(map(Marshall, convertnines))
```

```{r}
Marshall_temp <- mice(Marshall, m = 5, defaultmethod = c("pmm", "polyreg"), seed = 500)

Marshall_complete <- complete(Marshall_temp)

sum(is.na(Marshall_complete))
```
Because several of our categorical variables are integers we need to convert them to factors 
```{r}
masscoercion <- function(x){
  if (nlevels(as.factor(x))>1 & nlevels(as.factor(x))<8) {
    x <- as.factor(x)
 } else {
   x
   }
}

Marshall_complete<- as.data.frame(map(Marshall_complete, masscoercion))
```

```{r}
ggplot(data = Marshall, aes(satisfied))+geom_histogram(stat = "count")
```
##Run the Model 
Now that is done we can run the model
```{r}
library(caret)
library(randomForest)

set.seed(3000)
trainindex <- createDataPartition(Marshall_complete$satisfied, p = 0.8,
                                  list = FALSE,
                                  times = 1)

Marshall_train <- Marshall_complete[trainindex,]
Marshall_test <- Marshall_complete[-trainindex,]

Marshall.rf <- randomForest(satisfied~., data = Marshall_train, importance = TRUE)

plot(Marshall.rf)
Marshall.rf
varImpPlot(Marshall.rf, type = 2)
head(model.matrix(satisfied~.,data = Marshall_train))

test_prediction<-predict(Marshall.rf, Marshall_test[1:180])

confusionMatrix(data = test_prediction, reference = Marshall_test$satisfied)
```
Now we can tune the algorithm first I will try tools supplied by the randomforest package and then I will use the tools in the CARET package 
```{r}
set.seed(3000)

bestmtry <- tuneRF(Marshall_train[,-181], Marshall_train$satisfied, stepFactor=1.5, improve=1e-5, ntree=500, doBest = TRUE)

bestmtry

Marshall.rf_best <- randomForest(satisfied~., data = Marshall_train, importance = TRUE,   
                                 mtry = 19)

test_prediction_final <- predict(Marshall.rf_best, Marshall_test[1:180])

confusionMatrix(data = test_prediction_final, reference = Marshall_test$satisfied)
```
From above I was able to get an accuracy of 74% after tuning the model. My next step will be to try another model. 
